{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray-Beam is a library for writing [Apache Beam](http://beam.apache.org/) pipelines consisting of [xarray](http://xarray.pydata.org) Dataset objects. This tutorial (and Xarray-Beam itself) assumes basic familiarity with both Beam and Xarray.\n",
    "\n",
    "This tutorial will walk you through the basics of writing a pipeline with Xarray-Beam. We also recommend reading through a few [end to end examples](https://github.com/google/xarray-beam/tree/main/examples) to understand what code using Xarray-Beam typically looks like.\n",
    "\n",
    "```{note}\n",
    "Before getting started, it's important to understand that although Xarray-Beam tries to make it _straightforward_ to write distributed pipelines with Xarray objects, but it doesn't try to hide the distributed magic inside high-level objects like [Xarray with Dask](http://xarray.pydata.org/en/stable/user-guide/dask.html) or Dask/Spark DataFrames.\n",
    "\n",
    "Xarray-Beam is a lower-level tool. You will be manipulating large datasets piece-by-piece yourself, and you as the developer will be responsible for maintaining Xarray-Beam's internal invariants. This means that to successfully use Xarray-Beam, **you will need to understand how how it represents distributed datasets**. This may sound like a lot of responsibility, but we promise that isn't too bad!\n",
    "```\n",
    "\n",
    "We'll start off with some standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "import xarray_beam as xbeam\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys in Xarray-Beam\n",
    "\n",
    "Xarray-Beam is designed around the model that every stage in your Beam pipeline _could_ be stored in a single `xarray.Dataset` object, but is instead represented by a distributed beam `PCollection` of smaller `xarray.Dataset` objects, distributed in two possible ways:\n",
    "\n",
    "- Distinct _variables_ in a Dataset may be separated across multiple records.\n",
    "- Individual arrays can also be split into multiple _chunks_, similar to those used by [dask.array](https://docs.dask.org/en/latest/array.html).\n",
    "\n",
    "To keep track of how individual records could be combined into a larger (virtual) dataset, Xarray-Beam defines a `Key` object. Key objects consist of:\n",
    "\n",
    "1. `offsets`: integer offests for chunks from the origin in an `immutabledict`\n",
    "2. `vars`: The subset of variables included in each chunk, either as a `frozenset`, or as `None` to indicate \"all variables\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a `Key` from scratch is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Key(offsets={'x': 0, 'y': 10}, vars=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = xbeam.Key({'x': 0, 'y': 10}, vars=None)\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or given an existing `Key`, you can easily modify it with `replace()` or `with_offsets()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Key(offsets={'x': 0, 'y': 10}, vars={'bar', 'foo'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.replace(vars={'foo', 'bar'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Key(offsets={'y': 10, 'z': 1}, vars=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.with_offsets(x=None, z=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Key` objects don't do very much. They are just simple structs with two attributes, along with various special methods required to use them as `dict` keys or as keys in Beam pipelines. You can find a more examples of manipulating keys in the docstring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PCollections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard inputs & outputs for Xarray-Beam are PCollections of tuples of `(xbeam.Key, xarray.Dataset)` pairs. Xarray-Beam provides a bunch of PCollections for typical tasks, but many pipelines will still involve some manual manipulation of `Key` and `Dataset` objects, e.g., with builtin Beam transforms like `beam.Map`.\n",
    "\n",
    "To start off, let's write a helper functions for creating our first collection from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_records():\n",
    "    for offset in [0, 3]:\n",
    "        key = xbeam.Key({'x': offset})\n",
    "        chunk = xarray.Dataset({\n",
    "            'foo': ('x', offset + np.arange(3)),\n",
    "            'bar': ('x', 10 + offset + np.arange(3)),\n",
    "        })\n",
    "        yield key, chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look the entries, which are lazily constructed with the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(create_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2\n",
       "      bar      (x) int64 10 11 12),\n",
       " (Key(offsets={'x': 3}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 3 4 5\n",
       "      bar      (x) int64 13 14 15)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If desired, we could have set `vars={'foo', 'bar'}` on each of these `Key` objects instead of `vars=None`. This would be an equally valid representation of the same records, since all of our datasets have the same variables.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the inputs we need to use Xarray-Beam's helper functions and PTransforms. For example, we can fully consolidate chunks & variables to see what single `xarray.Dataset` these values would correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Key(offsets={'x': 0}, vars={'bar', 'foo'}),\n",
       " <xarray.Dataset>\n",
       " Dimensions:  (x: 6)\n",
       " Dimensions without coordinates: x\n",
       " Data variables:\n",
       "     foo      (x) int64 0 1 2 3 4 5\n",
       "     bar      (x) int64 10 11 12 13 14 15)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbeam.consolidate_fully(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute with Beam, of course, we need to turn Python lists/generators into Beam PCollections, typically with `beam.Create()`, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Key(offsets={'x': 0}, vars=None), <xarray.Dataset>\n",
      "Dimensions:  (x: 3)\n",
      "Dimensions without coordinates: x\n",
      "Data variables:\n",
      "    foo      (x) int64 0 1 2\n",
      "    bar      (x) int64 10 11 12)\n",
      "(Key(offsets={'x': 3}, vars=None), <xarray.Dataset>\n",
      "Dimensions:  (x: 3)\n",
      "Dimensions without coordinates: x\n",
      "Data variables:\n",
      "    foo      (x) int64 3 4 5\n",
      "    bar      (x) int64 13 14 15)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    p | beam.Create(create_records()) | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make use of transforms for \"rechunking\" how a dataset to be distributed in a different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest transformation is splitting (or consoldating) different _variables_ in a Dataset with `SplitVariables()` and `ConsolidateVariables()`, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2),\n",
       " (Key(offsets={'x': 0}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      bar      (x) int64 10 11 12),\n",
       " (Key(offsets={'x': 3}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 3 4 5),\n",
       " (Key(offsets={'x': 3}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      bar      (x) int64 13 14 15)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitVariables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also adjust _chunks_ in a dataset to distribute arrays of different sizes. Here you have two choices of API:\n",
    "\n",
    "1. The lower level {py:class}`~xarray_beam.SplitChunks` and {py:class}`~xarray_beam.ConsolidateChunks`. These transformations apply a single splitting (with indexing) or consolidation (with {py:function}`xarray.concat`) function to array elements.\n",
    "2. The high level {py:class}`~xarray_beam.Rechunk`, which uses a pipeline of multiple split/consolidate steps to efficient rechunk a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For minor adjustments, the more explicit `SplitChunks()` and `ConsolidateChunks()` are good options. They take a dict of _desired_ chunk sizes as a parameter, which can also be `-1` to indicate \"no chunking\" along a dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 6)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2 3 4 5\n",
       "      bar      (x) int64 10 11 12 13 14 15)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.ConsolidateChunks({'x': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because these transformations only split _or_ consolidate, they cannot necessary fully rechunk a dataset in a single step if the new chunk sizes are not multiples of old chunks (with consolidate) or do not even divide the old chunks (with split), e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2\n",
       "      bar      (x) int64 10 11 12),\n",
       " (Key(offsets={'x': 3}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 2)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 3 4\n",
       "      bar      (x) int64 13 14),\n",
       " (Key(offsets={'x': 5}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 1)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 5\n",
       "      bar      (x) int64 15)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5})  # notice that the first two chunks are still separate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such uneven cases, you'll need to use split followed by consolidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2 3 4\n",
       "      bar      (x) int64 10 11 12 13 14),\n",
       " (Key(offsets={'x': 5}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 1)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 5\n",
       "      bar      (x) int64 15)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5}) | xbeam.ConsolidateChunks({'x': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `Rechunk()` applies multiple split and consolidate steps based on the [Rechunker](https://github.com/pangeo-data/rechunker) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-f0f63827-5e6c-42f6-9eda-1f445e1ba489.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 0 1 2 3 4\n",
       "      bar      (x) int64 10 11 12 13 14),\n",
       " (Key(offsets={'x': 5}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 1)\n",
       "  Dimensions without coordinates: x\n",
       "  Data variables:\n",
       "      foo      (x) int64 5\n",
       "      bar      (x) int64 15)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.Rechunk(dim_sizes={'x': 6}, source_chunks={'x': 3}, target_chunks={'x': 5}, itemsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rechunk` requires specifying a few more parameters, but based on that information it can be _much_ more efficient for more complex rechunking tasks, particular in cases where data needs to be distributed into a very different shape (e.g., distributing a matrix across rows vs. columns). A naive \"splitting\" approach in such cases could divide datasets into extremely small tasks corresponding to individual array elements, which adds a huge amount of overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: finish this!\n",
    "\n",
    "- Discuss the nuances of feeding in Dask datasets into DatasetToChunks\n",
    "- Discuss options for lazy datasets: xarray's lazy indexing vs dask\n",
    "- ChunksToZarr (including `template`)\n",
    "- Fancy algorithms for rechunking"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef148d7ea0dbd1f91630322dd5bc9e24a2135d95f24fe1a9dab9696856be2b9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
