{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechunking lets us re-distribute how datasets are split between variables and chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll recreate our dummy data from the data model tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/apache_beam/__init__.py:79: UserWarning: This version of Apache Beam has not been sufficiently tested on Python 3.9. You may encounter bugs or missing features.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "import xarray_beam as xbeam\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_records():\n",
    "    for offset in [0, 4]:\n",
    "        key = xbeam.Key({'x': offset, 'y': 0})\n",
    "        data = 2 * offset + np.arange(8).reshape(4, 2)\n",
    "        chunk = xarray.Dataset({\n",
    "            'foo': (('x', 'y'), data),\n",
    "            'bar': (('x', 'y'), 100 + data),\n",
    "        })\n",
    "        yield key, chunk\n",
    "        \n",
    "inputs = list(create_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest transformation is splitting (or consoldating) different _variables_ in a Dataset with `SplitVariables()` and `ConsolidateVariables()`, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7),\n",
       " (Key(offsets={'x': 0, 'y': 0}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars={'foo'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 8 9 10 11 12 13 14 15),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars={'bar'}),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      bar      (x, y) int64 108 109 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitVariables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also adjust _chunks_ in a dataset to distribute arrays of different sizes. Here you have two choices of API:\n",
    "\n",
    "1. The lower level {py:class}`~xarray_beam.SplitChunks` and {py:class}`~xarray_beam.ConsolidateChunks`. These transformations apply a single splitting (with indexing) or consolidation (with {py:function}`xarray.concat`) function to array elements.\n",
    "2. The high level {py:class}`~xarray_beam.Rechunk`, which uses a pipeline of multiple split/consolidate steps to efficient rechunk a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For minor adjustments, the more explicit `SplitChunks()` and `ConsolidateChunks()` are good options. They take a dict of _desired_ chunk sizes as a parameter, which can also be `-1` to indicate \"no chunking\" along a dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 8, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 ... 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.ConsolidateChunks({'x': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because these transformations only split _or_ consolidate, they cannot necessary fully rechunk a dataset in a single step if the new chunk sizes are not multiples of old chunks (with consolidate) or do not even divide the old chunks (with split), e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 4, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107),\n",
       " (Key(offsets={'x': 4, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 1, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 8 9\n",
       "      bar      (x, y) int64 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5})  # notice that the first two chunks are still separate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such uneven cases, you'll need to use split followed by consolidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'ConsolidateChunks/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.SplitChunks({'x': 5}) | xbeam.ConsolidateChunks({'x': 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `Rechunk()` applies multiple split and consolidate steps based on the [Rechunker](https://github.com/pangeo-data/rechunker) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/shoyer/miniconda3/envs/xarray-beam/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/Users/shoyer/Library/Jupyter/runtime/kernel-2b08a4f7-b064-490e-9ac1-e22ceca4c6cd.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage0/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n",
      "WARNING:apache_beam.coders.coder_impl:Using fallback deterministic coder for type '<class 'xarray_beam._src.core.Key'>' in 'Rechunk/Stage2/Consolidate/GroupByTempKeys'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Key(offsets={'x': 0, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 5, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9\n",
       "      bar      (x, y) int64 100 101 102 103 104 105 106 107 108 109),\n",
       " (Key(offsets={'x': 5, 'y': 0}, vars=None),\n",
       "  <xarray.Dataset>\n",
       "  Dimensions:  (x: 3, y: 2)\n",
       "  Dimensions without coordinates: x, y\n",
       "  Data variables:\n",
       "      foo      (x, y) int64 10 11 12 13 14 15\n",
       "      bar      (x, y) int64 110 111 112 113 114 115)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs | xbeam.Rechunk(dim_sizes={'x': 6}, source_chunks={'x': 3}, target_chunks={'x': 5}, itemsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rechunk` requires specifying a few more parameters, but based on that information it can be _much_ more efficient for more complex rechunking tasks, particular in cases where data needs to be distributed into a very different shape (e.g., distributing a matrix across rows vs. columns). A naive \"splitting\" approach in such cases could divide datasets into extremely small tasks corresponding to individual array elements, which adds a huge amount of overhead."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef148d7ea0dbd1f91630322dd5bc9e24a2135d95f24fe1a9dab9696856be2b9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
